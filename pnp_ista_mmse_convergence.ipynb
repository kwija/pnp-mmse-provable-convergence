{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":31286,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":5,"nbformat":4,"cells":[{"id":"f62cb99c","cell_type":"markdown","source":"# Convergence de PnP-ISTA avec débruiteurs MMSE\n\nKHOUAJA Ahmed — M2 MIASHS, Lyon 2  \nCours Problèmes Inverses (N. Pustelnik)  \nArticle : Xu, Sun, Liu, Wohlberg, Kamilov — *IEEE SPL*, 2020\n\n---","metadata":{}},{"id":"4640d03e","cell_type":"markdown","source":"## 1. Introduction\n\nEn traitement du signal, on cherche à reconstruire un signal $\\mathbf{x}$ à partir de mesures dégradées $\\mathbf{y} = H\\mathbf{x} + \\mathbf{e}$, où $H$ modélise l'acquisition et $\\mathbf{e}$ le bruit. Ce problème est mal posé et on régularise pour obtenir une solution — c'est le sujet des premiers chapitres du cours.\n\nL'approche Plug-and-Play (Venkatakrishnan et al. 2013) remplace le régulariseur par un débruiteur dans les itérations d'un algorithme proximal. L'article étudié ici prouve la convergence de PnP-ISTA quand le débruiteur est un estimateur MMSE. Jusqu'ici, les preuves de convergence supposaient la nonexpansivité du débruiteur, ce qui n'est pas le cas du MMSE. Xu et al. contournent le problème via une approche de majorisation-minimisation.","metadata":{}},{"id":"fafa963f","cell_type":"code","source":"import numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.stats import norm\n\nnp.random.seed(2024)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-24T14:24:01.240232Z","iopub.execute_input":"2026-02-24T14:24:01.240515Z","iopub.status.idle":"2026-02-24T14:24:02.976033Z","shell.execute_reply.started":"2026-02-24T14:24:01.240480Z","shell.execute_reply":"2026-02-24T14:24:02.974916Z"}},"outputs":[],"execution_count":1},{"id":"613e8074","cell_type":"markdown","source":"## 2. Cadre théorique : ISTA et Plug-and-Play\n\nLa reconstruction régularisée revient à résoudre $\\hat{\\mathbf{x}} = \\arg\\min_{\\mathbf{x}} g(\\mathbf{x}) + h(\\mathbf{x})$ avec $g(\\mathbf{x}) = \\frac{1}{2}\\|\\mathbf{y} - H\\mathbf{x}\\|^2$ et $h$ un régulariseur ($\\ell_1$, TV, etc.).\n\nL'algorithme ISTA, ou Forward-Backward (chap. 4 du cours), itère :\n$$z_t = x_{t-1} - \\gamma \\nabla g(x_{t-1}), \\quad x_t = \\mathrm{prox}_{\\gamma h}(z_t)$$\navec $\\gamma \\leq 1/L$, $L$ constante de Lipschitz de $\\nabla g$. L'opérateur proximal admet une interprétation bayésienne : si $h = -\\log p_x$, le proximal correspond au débruiteur MAP pour un bruit gaussien de variance $\\gamma$. L'approche PnP remplace ce proximal par un débruiteur $D_\\sigma$ :\n\n$$z_t = x_{t-1} - \\gamma \\nabla g(x_{t-1}), \\qquad x_t = D_\\sigma(z_t)$$\n\nLe débruiteur MAP est nonexpansif et les preuves de convergence existantes reposent sur cette propriété (Chan 2017, Ryu 2019). Le débruiteur MMSE ($\\mathbb{E}[\\mathbf{x}|\\mathbf{z}]$), qui minimise l'erreur quadratique, est en général expansif. Or la majorité des débruiteurs utilisés en pratique — DnCNN entraîné en MSE, BM3D — sont des approximations du MMSE. La formule de Tweedie $D_\\sigma(\\mathbf{z}) = \\mathbf{z} + \\sigma^2 \\nabla \\log p_z(\\mathbf{z})$ permet de montrer que le MMSE est le proximal d'un $h_\\mathrm{mmse}$ possiblement non convexe (Gribonval 2011).","metadata":{}},{"id":"8de26702","cell_type":"markdown","source":"## 3. Résultat de convergence\n\nSous trois hypothèses — prior non-dégénéré, gradient $L$-Lipschitz, $f = g+h_\\mathrm{mmse}$ bornée inférieurement — la suite $\\{f(x_t)\\}$ produite par PnP-ISTA avec le MMSE décroît de façon monotone et $\\|\\nabla f(x_t)\\| \\to 0$. La preuve passe par un majorant quadratique :\n$$\\bar\\mu(x,s) = g(s) + \\langle \\nabla g(s), x-s\\rangle + \\frac{1}{2\\gamma}\\|x-s\\|^2 + h_\\mathrm{mmse}(x)$$\nComme $D_\\sigma = \\mathrm{prox}_{\\gamma h_\\mathrm{mmse}}$, chaque itération minimise exactement ce majorant. La suite, décroissante et bornée, converge. Pas besoin de convexité ni de nonexpansivité.","metadata":{}},{"id":"26061923","cell_type":"markdown","source":"## 4. Résultats de l'article\n\nLes auteurs testent sur du compressive sensing : signal Bernoulli-Gaussien ($n=4096$, $\\alpha = 0.1$), matrice gaussienne i.i.d., 20 dB de SNR.\n\nLa convergence monotone de $f(x_t)/f(x_0)$ est vérifiée. Le DnCNN entraîné en MSE reproduit le MMSE analytique, confirmant que ces réseaux sont bien des approximations MMSE. PnP-ISTA reste en dessous de GAMP, ce qui s'explique par le fait qu'ISTA suppose un bruit AWGN dans les résidus, hypothèse fausse en pratique.","metadata":{}},{"id":"847bdeb2","cell_type":"markdown","source":"## 5. Reproduction\n\nDeux expériences : compressive sensing 1D avec le MMSE analytique, puis défloutage 2D avec DnCNN via DeepInverse. On travaille en dimension réduite par rapport à l'article.","metadata":{}},{"id":"4f4e5c50","cell_type":"markdown","source":"### 5.1 Compressive sensing avec MMSE exact\n\nPour un prior Bernoulli-Gaussien i.i.d., le débruiteur MMSE s'exprime analytiquement composante par composante : $D_\\sigma(z_i) = w(z_i) \\frac{\\sigma_x^2}{\\sigma^2 + \\sigma_x^2} z_i$ avec $w(z_i) = P(x_i \\neq 0 \\mid z_i)$.","metadata":{}}]}